{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement highres3dnet\n",
    "\n",
    "Paper: https://arxiv.org/abs/1707.01992\n",
    "\n",
    "Preprocessing steps\n",
    "---\n",
    "- Anatomical\n",
    "    1. Load.\n",
    "    1. Squeeze data array.\n",
    "    1. Check for 3 dimensions.\n",
    "    1. Add one color channel.\n",
    "- Labels\n",
    "    1. Load.\n",
    "    1. Squeeze data array.\n",
    "    1. Check for 3 dimensions.\n",
    "    1. One-hot encode.\n",
    "   \n",
    "Loading steps\n",
    "---\n",
    "1. Load anatomical and labels.\n",
    "1. Get blocks for each array.\n",
    "1. Feed those blocks into the model in batches.\n",
    "\n",
    "Questions\n",
    "---\n",
    "1. Should we impose a restriction, where batch size must be evenly divisible by number of blocks (viewpoints) that are in a volume? Probably.\n",
    "\n",
    "Todo\n",
    "---\n",
    "- Look into [`keras.utils.multi_gpu_model`](https://keras.io/utils/#multi_gpu_model) to train on multiple GPUs. This is only available for the TensorFlow backend.\n",
    "- Learn about one-hot encoding / decoding. Encode the array of labels with `K.one_hot()`. Decode the predictions with `K.argmax()`. `K.one_hot()` simply calls `tf.one_hot()`, so this would restrict us to the TensorFlow backend, which is fine for now.\n",
    "\n",
    "\n",
    "Notes\n",
    "---\n",
    "- NiftyNet trains on a sliding window over the 3D data. This should have that ability to lower GPU memory requirements.\n",
    "\n",
    "Future considerations\n",
    "---\n",
    "- Modify `highres3dnet` to use ResNeXt architechture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from warnings import warn\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from highres3dnet import dice_loss, HighRes3DNet\n",
    "\n",
    "logger = logging.getLogger(name=__name__)\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "CSV_FILEPATH = \"/om2/user/jakubk/openmind-surface-data/file-lists/master_file_list_brainmask.csv\"\n",
    "# CSV_FILEPATH = \"/om/user/jakubk/nobrainer-code/niftynet_to_keras/t1_brainmask.csv\"\n",
    "WINDOW_SHAPE = (128, 128, 128)\n",
    "NUM_CHANNELS = 1\n",
    "INPUT_SHAPE = (*WINDOW_SHAPE, NUM_CHANNELS)\n",
    "TARGET_DTYPE = 'uint8'\n",
    "TENSORBOARD_BASE_DIR = \"/om/user/jakubk/nobrainer-code/niftynet_to_keras/models\"\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_timestamp():\n",
    "    import datetime\n",
    "    return str(datetime.datetime.now()).split('.')[0].replace(' ', '_')\n",
    "\n",
    "\n",
    "def get_tensorboard_dir(base_dir=None):\n",
    "    if base_dir is None:\n",
    "        base_dir = os.getcwd()\n",
    "    window = \"_\".join(str(ii) for ii in WINDOW_SHAPE)\n",
    "    rel_dir = (\n",
    "        \"highres3dnet-{num_classes}_classes-{lr}_lr-{batch}_batch-{window}_window-{ts}\"\n",
    "    ).format(\n",
    "        num_classes=NUM_CLASSES, lr=LEARNING_RATE, batch=BATCH_SIZE, \n",
    "        window=window, ts=_get_timestamp())\n",
    "    return os.path.join(base_dir, rel_dir, 'logs')\n",
    "\n",
    "\n",
    "def load_volume(filepath, return_affine=False, c_contiguous=True, dtype=None):\n",
    "    \"\"\"Return data given filepath to volume. Optionally return affine array.\n",
    "\n",
    "    Making the data array contiguous takes more time during loading, but this\n",
    "    ultimately saves time when viewing blocks of data with `skimage`.\n",
    "    \"\"\"\n",
    "    img = nib.load(filepath)\n",
    "    data = np.asarray(img.dataobj)\n",
    "    if dtype is not None:\n",
    "        data = data.astype(dtype)\n",
    "    img.uncache()\n",
    "    if c_contiguous:\n",
    "        data = np.ascontiguousarray(data)\n",
    "    if return_affine:\n",
    "        return data, img.affine\n",
    "    return data\n",
    "\n",
    "\n",
    "def one_hot(a, **kwargs):\n",
    "    \"\"\"Return one-hot array of N-D array `a`.\"\"\"\n",
    "    # https://stackoverflow.com/a/37323404/5666087\n",
    "    n_values = int(np.max(a) + 1)\n",
    "    return np.eye(n_values, **kwargs)[a]\n",
    "\n",
    "\n",
    "def _preprocess_data(data):\n",
    "    data = view_as_blocks(data, WINDOW_SHAPE).reshape(-1, *WINDOW_SHAPE)\n",
    "    return data[Ellipsis, np.newaxis]\n",
    "\n",
    "\n",
    "def _preprocess_target(target):\n",
    "    target = one_hot(target, dtype=TARGET_DTYPE)\n",
    "    new_shape = (*WINDOW_SHAPE, NUM_CLASSES)\n",
    "    return view_as_blocks(target, new_shape).reshape(-1, *new_shape)\n",
    "\n",
    "\n",
    "def view_as_blocks(arr_in, block_shape):\n",
    "    \"\"\"Block view of the input n-dimensional array (using re-striding).\n",
    "    Blocks are non-overlapping views of the input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr_in : ndarray\n",
    "        N-d input array.\n",
    "    block_shape : tuple\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Copied from `skimage.util.view_as_blocks` to avoid having to install the\n",
    "    entire package + dependencies.\n",
    "    \"\"\"\n",
    "    if not isinstance(block_shape, tuple):\n",
    "        raise TypeError('block needs to be a tuple')\n",
    "\n",
    "    block_shape = np.array(block_shape)\n",
    "    if (block_shape <= 0).any():\n",
    "        raise ValueError(\"'block_shape' elements must be strictly positive\")\n",
    "\n",
    "    if block_shape.size != arr_in.ndim:\n",
    "        raise ValueError(\"'block_shape' must have the same length \"\n",
    "                         \"as 'arr_in.shape'\")\n",
    "\n",
    "    arr_shape = np.array(arr_in.shape)\n",
    "    if (arr_shape % block_shape).sum() != 0:\n",
    "        raise ValueError(\"'block_shape' is not compatible with 'arr_in'\")\n",
    "\n",
    "    # -- restride the array to build the block view\n",
    "\n",
    "    if not arr_in.flags.contiguous:\n",
    "        warn(RuntimeWarning(\"Cannot provide views on a non-contiguous input \"\n",
    "                            \"array without copying.\"))\n",
    "\n",
    "    arr_in = np.ascontiguousarray(arr_in)\n",
    "\n",
    "    new_shape = tuple(arr_shape // block_shape) + tuple(block_shape)\n",
    "    new_strides = tuple(arr_in.strides * block_shape) + arr_in.strides\n",
    "\n",
    "    arr_out = as_strided(arr_in, shape=new_shape, strides=new_strides)\n",
    "\n",
    "    return arr_out\n",
    "\n",
    "\n",
    "def blocks_to_volume(arr):\n",
    "    \"\"\"Combine 4D array of non-overlapping cubes to 3D cube array.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> mask.shape\n",
    "    (8, 128, 128, 128)\n",
    "    >>> blocks_to_volume(mask).shape\n",
    "    (256, 256, 256)\n",
    "    \"\"\"\n",
    "    n_blocks = arr.shape[0]\n",
    "    new_ndim = arr.ndim - 1\n",
    "    nn = int(n_blocks ** (1 / new_ndim))\n",
    "    new_shape = (nn * arr.shape[1],) * new_ndim\n",
    "    return (\n",
    "        arr.reshape(nn, nn, nn, *arr.shape[1:])\n",
    "        .transpose(0, 3, 1, 4, 2, 5)\n",
    "        .reshape(new_shape)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_input = pd.read_csv(CSV_FILEPATH)\n",
    "\n",
    "model = HighRes3DNet(n_classes=NUM_CLASSES, input_shape=INPUT_SHAPE)\n",
    "\n",
    "# Use multiple GPUs.\n",
    "# gpu_ids = [int(ss) for ss in os.environ['CUDA_VISIBLE_DEVICES'].split(',')]\n",
    "# model = keras.utils.multi_gpu_model(model, gpus=gpu_ids)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "model.compile(adam, dice_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/issues/5935#issuecomment-289041967\n",
    "class MemoryCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, log={}):\n",
    "        import resource\n",
    "        # max resident set size\n",
    "        usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "        usage = usage * resource.getpagesize() / 1000000.0\n",
    "        print(\"Usage: {:0.0f} Mb\".format(usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_tensorboard_dir = get_tensorboard_dir(base_dir=TENSORBOARD_BASE_DIR)\n",
    "\n",
    "print(\"++ Saving Tensorboard information to\\n{}\".format(_tensorboard_dir))\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=_tensorboard_dir,\n",
    "        write_graph=False,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(_tensorboard_dir, \"..\", \"model-{epoch:02d}.h5\"),\n",
    "        period=50,\n",
    "    ),\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        filename=os.path.join(_tensorboard_dir, \"..\", \"training.log\"),\n",
    "        append=True,\n",
    "    ),\n",
    "    MemoryCallback(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, these_files in df_input.iterrows():\n",
    "    \n",
    "    data = load_volume(these_files['t1'])\n",
    "    data = _preprocess_data(data)\n",
    "    \n",
    "    target = load_volume(these_files['brainmask'], dtype=TARGET_DTYPE)\n",
    "    target = _preprocess_target(target)\n",
    "    \n",
    "    model.fit(\n",
    "        x=data,\n",
    "        y=target,\n",
    "        epochs=1,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=1,\n",
    "        # callbacks=callbacks\n",
    "    )\n",
    "    if index > 25:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run on CPU only.\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from warnings import warn\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from highres3dnet import dice_loss, HighRes3DNet\n",
    "\n",
    "logger = logging.getLogger(name=__name__)\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.001\n",
    "CSV_FILEPATH = \"/om/user/jakubk/nobrainer-code/niftynet_to_keras/t1_brainmask.csv\"\n",
    "WINDOW_SHAPE = (128, 128, 128)\n",
    "NUM_CHANNELS = 1\n",
    "INPUT_SHAPE = (*WINDOW_SHAPE, NUM_CHANNELS)\n",
    "TARGET_DTYPE = 'uint8'\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    'models/highres3dnet-2_classes-0.0001_lr-1_batch-128_128_128_window-2018-01-29_12:57:45/model-00.h5', \n",
    "    custom_objects={'dice_loss': dice_loss}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindboggle_data = pd.read_csv('/om2/user/jakubk/mindboggle-101/all-101-files.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = np.zeros((256, 256, 256), dtype='<i2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = anat_true.copy()\n",
    "foo.resize(256, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ii = 0\n",
    "anat_true, affine_anat_true = load_volume(mindboggle_data.loc[ii, 't1'], return_affine=True)\n",
    "anat_true = np.pad(anat_true, ((0,0),(0,0),(48, 48)), 'constant')\n",
    "anat_true_orig = anat_true.copy()\n",
    "anat_true = _preprocess_data(anat_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(anat_true_orig[:, 150, :], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "prediction = model.predict(anat_true)\n",
    "diff = time.time() - t0\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mask = prediction.argmax(-1)\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = blocks_to_volume(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_img = nib.Nifti1Image(out, affine_anat_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_img.to_filename('testout.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative reshape methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "anatp = anat_proc.squeeze()\n",
    "anatp.shape\n",
    "\n",
    "def permute_axes(a, div_len_by=2):\n",
    "    shp1 = np.hstack([(i // div_len_by, 2) for i in a.shape])\n",
    "    shp2 = [8,] + [i // div_len_by for i in a.shape]\n",
    "    a = a.reshape(shp1)\n",
    "    print(\"after reshape1\", a.shape)\n",
    "    a = a.transpose(0,2,4,1,3,5)\n",
    "    print(\"after transpose\", a.shape)\n",
    "    a = a.reshape(shp2)\n",
    "    print(\"after reshape2\", a.shape)\n",
    "    return a\n",
    "\n",
    "permute_axes(anat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new = (\n",
    "    anatp.reshape((128, 128, 128, 2, 2, 2))\n",
    "    .transpose(0, 3, 1, 4, 2, 5)\n",
    "    .reshape(256, 256, 256)\n",
    ")\n",
    "print(new.shape)\n",
    "\n",
    "assert new.shape == (256,256,256), \"Incorrect shape\"\n",
    "\n",
    "plt.matshow(new[:, 150, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"{x} {y} {z}\"\n",
    "d = {\"x\":3, \"y\":\"blah\", \"z\":0}\n",
    "a.format(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot for aparc+aseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aparcaseg_csv = \"/om2/user/jakubk/openmind-surface-data/file-lists/master_file_list_aparcaseg.csv\"\n",
    "aparcaseg_mapping_csv = \"/om2/user/jakubk/openmind-surface-data/data/FreeSurferColorLUT-mapping-108.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(aparcaseg_csv)\n",
    "aparcaseg_mapping = pd.read_csv(aparcaseg_mapping_csv, index_col='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_volume(df.loc[10, 'aparcaseg'], dtype='uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mapping(filepath):\n",
    "    import csv\n",
    "    with open(aparcaseg_mapping_csv) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        return {int(aa[0]): int(aa[1]) for aa in reader if aa[0] != 'label'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = _get_mapping(aparcaseg_mapping_csv)\n",
    "mapping_values_arr = np.array(list(mapping.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values of array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero any values not in the list of labels we are using.\n",
    "_not_in_mappin_mask = ~np.isin(data, mapping_keys_arr)\n",
    "data[_not_in_mappin_mask] = 0\n",
    "\n",
    "# Convert to range [0, 107].\n",
    "for orig, new in mapping.items():\n",
    "    data[data==orig] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
