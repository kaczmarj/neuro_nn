{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to include in tutorial\n",
    "\n",
    "- make nobrainer installable via pip, conda\n",
    "- create a set of google collaboratory demos\n",
    "   - how to create a network and train it on the tpu\n",
    "   - how to insert data augmentation\n",
    "   - how to use transfer learning on a trained network\n",
    "- release a set of niflow applications\n",
    "   - brain extraction\n",
    "   - brain segmentation\n",
    "   - tumor labeling\n",
    "- release the corresponding set of trained models (with details on which data it was trained on)\n",
    "- release patrick's DWC PR and see if we can get the uncertainty mapping  (by march 7)\n",
    "   - it would be great if we can work with patrick to basically have his version of the brain extraction and segmentation together with uncertainty images available as applications and pre-trained models. this would be in addition to the ones you have trained.\n",
    "- if possible, a Web version of the brain segmentor that runs on a nifti image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Nobrainer\n",
    "\n",
    "Nobrainer can be installed using `pip`. Use the extra `[gpu]` to install TensorFlow with GPU support or the extra `[cpu]` to install TensorFlow without GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir https://github.com/kaczmarj/nobrainer-tf2/tarball/dev tf-nightly-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Nobrainer\n",
    "\n",
    "## Command-line\n",
    "\n",
    "Nobrainer provides the command-line program `nobrainer`, which contains various methods for preparing data, training and evaluating models, generating predictions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nobrainer convert --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMP: use !nobrainer convert --help\n",
    "!PYTHONPATH='..' python -m nobrainer.cli.main --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python\n",
    "\n",
    "The `nobrainer` Python package can be imported as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMP\n",
    "import sys; sys.path.append('..'); del sys\n",
    "\n",
    "import nobrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data\n",
    "\n",
    "You will need a nested list of features and labels volumes. One can store this as a CSV that looks like the following:\n",
    "\n",
    "```\n",
    "features,labels\n",
    "/path/to/1_features.nii.gz,/path/to/1_labels.nii.gz\n",
    "/path/to/2_features.nii.gz,/path/to/2_labels.nii.gz\n",
    "/path/to/3_features.nii.gz,/path/to/3_labels.nii.gz\n",
    "/path/to/4_features.nii.gz,/path/to/4_labels.nii.gz\n",
    "```\n",
    "\n",
    "and then read it with `nobrainer.io.read_csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sample data\n",
    "\n",
    "Here, we download 10 T1-weighted brain scans and their corresponding FreeSurfer segmentations. These volumes take up about 46 MB and are saved to a temporary directory. The object `volume_filepaths` is a list of tuples, where each tuple hold the path to the features volume and the path to the corresponding labels volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = nobrainer.utils.get_data()\n",
    "\n",
    "!head $csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to volume files to TFRecords\n",
    "\n",
    "To achieve the best performance, training data should be in TFRecords format. This is the preferred file format for TensorFlow, Training can be done on medical imaging volume files but will be slower.\n",
    "\n",
    "Nobrainer has a command-line utility to convert volumes to TFRecords: `nobrainer convert`. This will verify that all of the volumes have the same shape and that the label volumes are an integer type or can be safely coerced to an integer type. \n",
    "\n",
    "Following successful verification, the volumes will be converted to TFRecords files. The dataset should be sharded into multiple TFRecords files, so that data can be shuffled more properly. This is especially helpful for large datasets. Users can choose how many pairs of volumes (i.e., features and labels) will be saved to one TFRecords file. In this example, we will save 3 pairs of volumes per TFRecords file because our dataset is small. With a larger dataset, users should choose a larger shard value. For example, with 10,000 volumes, one might choose 100 volumes per TFRecords file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TMP: use !nobrainer convert --help\n",
    "!PYTHONPATH='..' python -m nobrainer.cli.main convert --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMP: use !nobrainer convert --help\n",
    "!PYTHONPATH='..' python -m nobrainer.cli.main convert \\\n",
    "    --csv='/tmp/nobrainer-data/filepaths.csv' \\\n",
    "    --tfrecords-template='tfrecords/data_shard-{shard:03d}.tfrecords' \\\n",
    "    --volumes-per-shard=4 \\\n",
    "    --volume-shape 256 256 256 \\\n",
    "    --num-parallel-calls=8 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input data pipeline\n",
    "\n",
    "We will now create an data pipeline to feed our models with training data. The steps below will create a `tensorflow.data.Dataset` object that is built according to [TensorFlow's guidelines](https://www.tensorflow.org/guide/performance/datasets). The basic pipeline is summarized below.\n",
    "\n",
    "- Read data\n",
    "- Separate volumes into non-overlapping sub-volumes\n",
    "    - This is done to get around memory limitations with larger models.\n",
    "    - For example, a volume with shape (256, 256, 256) can be separated into eight non-overlapping blocks of shape (128, 128, 128).\n",
    "- Apply random rigid augmentations if requested.\n",
    "- Standard score volumes of features.\n",
    "- Binarize labels if binary segmentation.\n",
    "- Replace values according to some mapping if multi-class segmentation.\n",
    "- Batch the results so every iteration yields `batch_size` elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glob pattern to match the files we want to train on.\n",
    "file_pattern = 'tfrecords/data_shard-*.tfrecords'\n",
    "\n",
    "# The number of classes the model predicts. A value of 1 means the model performs\n",
    "# binary classification (i.e., target vs background).\n",
    "n_classes = 1\n",
    "\n",
    "# Batch size is the number of features and labels we train on with each step.\n",
    "batch_size = 2\n",
    "\n",
    "# The shape of the original volumes.\n",
    "volume_shape = (256, 256, 256)\n",
    "\n",
    "# The shape of the non-overlapping sub-volumes. Most models cannot be trained on\n",
    "# full volumes because of hardware and memory constraints, so we train and evaluate\n",
    "# on sub-volumes.\n",
    "block_shape = (128, 128, 128)\n",
    "\n",
    "# Whether or not to apply random rigid transformations to the data on the fly.\n",
    "# This can improve model generalizability but increases processing time.\n",
    "augment = False\n",
    "\n",
    "# The tfrecords filepaths will be shuffled before reading, but we can also shuffle\n",
    "# the data. This will shuffle 10 volumes at a time. Larger buffer sizes will require\n",
    "# more memory, so choose a value based on how much memory you have available.\n",
    "shuffle_buffer_size = 10\n",
    "\n",
    "# Number of parallel processes to use.\n",
    "num_parallel_calls = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls $file_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = nobrainer.volume.get_dataset(\n",
    "    file_pattern=file_pattern,\n",
    "    n_classes=n_classes,\n",
    "    batch_size=batch_size,\n",
    "    volume_shape=volume_shape,\n",
    "    block_shape=block_shape,\n",
    "    augment=augment,\n",
    "    n_epochs=1,\n",
    "    shuffle_buffer_size=shuffle_buffer_size,\n",
    "    num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a pre-defined `nobrainer` model\n",
    "\n",
    "Users can find pre-defined models under the namespace `nobrainer.models`. All models are implemented using the `tf.keras` API, which makes definitions highly readable and hackable, despite being a high-level interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = nobrainer.models.unet(n_classes=n_classes, input_shape=(*block_shape, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot the model here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "All Keras models must be compiled before they can be trained. This is where you choose the loss function and any other metrics that should be reported during training. Nobrainer implements several loss functions useful for semantic segmentation, including Dice, Generalized Dice, Jaccard, and Tversky losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-04),\n",
    "    loss=nobrainer.losses.jaccard,\n",
    "    metrics=[nobrainer.metrics.dice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a single GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = nobrainer.volume.get_steps_per_epoch(\n",
    "    n_volumes=10, \n",
    "    volume_shape=volume_shape, \n",
    "    block_shape=block_shape, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on multiple GPUs on the same machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = nobrainer.models.unet(n_classes=n_classes, input_shape=(*block_shape, 1))\n",
    "    model.compile(\n",
    "        optimizer=tf.train.AdamOptimizer(1e-04),\n",
    "        loss=nobrainer.losses.jaccard,\n",
    "        metrics=[nobrainer.metrics.dice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can train on double the amount of data. With 4 1080Ti GPUs, one can effectively train\n",
    "# on an entire volume at a time!\n",
    "batch_size = 4\n",
    "    \n",
    "dataset = nobrainer.volume.get_dataset(\n",
    "    file_pattern=file_pattern,\n",
    "    n_classes=n_classes,\n",
    "    batch_size=batch_size,\n",
    "    volume_shape=volume_shape,\n",
    "    block_shape=block_shape,\n",
    "    augment=augment,\n",
    "    n_epochs=1,\n",
    "    shuffle_buffer_size=shuffle_buffer_size,\n",
    "    num_parallel_calls=num_parallel_calls\n",
    ")\n",
    "\n",
    "steps_per_epoch = nobrainer.volume.get_steps_per_epoch(\n",
    "    n_volumes=10, \n",
    "    volume_shape=volume_shape, \n",
    "    block_shape=block_shape, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on TPU\n",
    "\n",
    "For a guide on how to train on TPUs (available via Google Cloud), please see the notebook [02-train_on_tpu.ipynb](02-train_on_tpu.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
