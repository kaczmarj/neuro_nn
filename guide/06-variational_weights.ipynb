{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMP\n",
    "import sys; sys.path.append('..'); del sys\n",
    "\n",
    "import nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A glob pattern to match the files we want to train on.\n",
    "file_pattern = 'tfrecords/data_shard-*.tfrecords'\n",
    "\n",
    "# The number of classes the model predicts. A value of 1 means the model performs\n",
    "# binary classification (i.e., target vs background).\n",
    "n_classes = 1\n",
    "\n",
    "# Batch size is the number of features and labels we train on with each step.\n",
    "batch_size = 4\n",
    "\n",
    "# The shape of the original volumes.\n",
    "volume_shape = (256, 256, 256)\n",
    "\n",
    "# The shape of the non-overlapping sub-volumes. Most models cannot be trained on\n",
    "# full volumes because of hardware and memory constraints, so we train and evaluate\n",
    "# on sub-volumes.\n",
    "block_shape = (64, 64, 64)\n",
    "\n",
    "# Whether or not to apply random rigid transformations to the data on the fly.\n",
    "# This can improve model generalizability but increases processing time.\n",
    "augment = False\n",
    "\n",
    "# The tfrecords filepaths will be shuffled before reading, but we can also shuffle\n",
    "# the data. This will shuffle 10 volumes at a time. Larger buffer sizes will require\n",
    "# more memory, so choose a value based on how much memory you have available.\n",
    "shuffle_buffer_size = 0\n",
    "\n",
    "# Number of parallel processes to use.\n",
    "num_parallel_calls = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = nobrainer.volume.get_dataset(\n",
    "    file_pattern=file_pattern,\n",
    "    n_classes=n_classes,\n",
    "    batch_size=batch_size,\n",
    "    volume_shape=volume_shape,\n",
    "    block_shape=block_shape,\n",
    "    augment=augment,\n",
    "    n_epochs=None,\n",
    "    shuffle_buffer_size=shuffle_buffer_size,\n",
    "    num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = nobrainer.volume.get_steps_per_epoch(\n",
    "    n_volumes=10, \n",
    "    volume_shape=volume_shape, \n",
    "    block_shape=block_shape, \n",
    "    batch_size=batch_size)\n",
    "\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from nobrainer.layers import VariationalConv3D\n",
    "\n",
    "\n",
    "def meshnet(n_classes, input_shape, filters=21, activation='relu', batch_size=None, name='meshnet'):\n",
    "\n",
    "    def one_layer(x, layer_num, dilation_rate=(1, 1, 1)):\n",
    "        x = VariationalConv3D(filters, kernel_size=(3, 3, 3), padding='same', dilation_rate=dilation_rate, name='layer{}/conv3d'.format(layer_num))(x)\n",
    "        x = layers.BatchNormalization(name='layer{}/batchnorm'.format(layer_num))(x)\n",
    "        x = layers.Activation(activation, name='layer{}/activation'.format(layer_num))(x)\n",
    "        # x = layers.Dropout(dropout_rate, name='layer{}/dropout'.format(layer_num))(x)\n",
    "        return x\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape, batch_size=batch_size, name='inputs')\n",
    "\n",
    "    x = one_layer(inputs, 1)\n",
    "    x = one_layer(x, 2)\n",
    "    x = one_layer(x, 3, dilation_rate=(2, 2, 2))\n",
    "    x = one_layer(x, 4, dilation_rate=(4, 4, 4))\n",
    "    x = one_layer(x, 5, dilation_rate=(8, 8, 8))\n",
    "    x = one_layer(x, 6, dilation_rate=(16, 16, 16))\n",
    "    x = one_layer(x, 7)\n",
    "\n",
    "    x = VariationalConv3D(filters=n_classes, kernel_size=(1, 1, 1), padding='same', name='classification/conv3d')(x)\n",
    "\n",
    "    final_activation = 'sigmoid' if n_classes <= 2 else 'softmax'\n",
    "    x = layers.Activation(final_activation, name='classification/activation')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x, name=name)\n",
    "\n",
    "\n",
    "model = meshnet(1, (*block_shape, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(1e-04), 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_get_layer_attr(model, weight_attribute):\n",
    "    \"\"\"Get the weight attribute\"\"\"\n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        try:\n",
    "            this_weight = getattr(layer, weight_attribute)\n",
    "            weights.append(this_weight)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VWLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, model, other_loss_callable, priors=None, n_examples=1, only_kld=False):\n",
    "        super(VWLoss, self).__init__(reduction=tf.losses.Reduction.SUM_OVER_NONZERO_WEIGHTS, name='vwloss')\n",
    "        self.model = model\n",
    "        self.other_loss_callable = other_loss_callable\n",
    "        self.priors = priors\n",
    "        self.n_examples = n_examples\n",
    "        self.only_kld = only_kld\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        \n",
    "        ms = _maybe_get_layer_attr(model, 'kernel_m') \\\n",
    "                + _maybe_get_layer_attr(model, 'bias_m')\n",
    "        sigmas = _maybe_get_layer_attr(model, 'kernel_sigma') \\\n",
    "                + _maybe_get_layer_attr(model, 'bias_sigma')\n",
    "        if self.priors is None:\n",
    "            ms_prior = [\n",
    "                tf.constant(1, dtype=v.dtype, shape=v.shape)\n",
    "                for v in ms]\n",
    "            sigmas_prior = [\n",
    "                tf.constant(1, dtype=v.dtype, shape=v.shape)\n",
    "                for v in ms]\n",
    "        else:\n",
    "            ms_prior = [\n",
    "                tf.Variable(tf.convert_to_tensor(self.priors[1][i]), trainable=False) \n",
    "                for i, _ in enumerate(ms)]\n",
    "            sigmas_prior = [\n",
    "                tf.Variable(tf.convert_to_tensor(self.priors[1][i]), trainable=False)\n",
    "                for i, _ in enumerate(ms)]\n",
    "            \n",
    "        nll_loss = self.other_loss_callable(y_true=y_true, y_pred=y_pred)\n",
    "        l2_loss = tf.add_n(\n",
    "            [\n",
    "                tf.reduce_sum((tf.square(ms[i] - ms_prior[i])) / ((tf.square(sigmas_prior[i]) + 1e-8) * 2.0)) \n",
    "                for i, _ in enumerate(ms)], name='l2_loss')\n",
    "        \n",
    "        sigma_squared_loss = tf.add_n([tf.reduce_sum(tf.square(sigmas[i]) / ((tf.square(sigmas_prior[i]) + 1e-8) * 2.0)) for i in range(len(sigmas))],name = 'sigma_squared_loss')\n",
    "        log_sigma_loss = tf.add_n([tf.reduce_sum(tf.log(v+1e-8)) for v in sigmas],name='log_sigmas_loss')\n",
    "        \n",
    "        if not self.only_kld:\n",
    "            loss = nll_loss + (l2_loss + sigma_squared_loss - log_sigma_loss) / float(self.n_examples)\n",
    "        else:\n",
    "            mse_m_loss = tf.add_n([tf.reduce_sum(tf.square(ms[i] - ms_prior[i])) for i in range(len(ms))], name='mse_m_loss')\n",
    "            mse_sigmas_loss = tf.add_n([tf.reduce_sum(tf.square(sigmas[i] - sigmas_prior[i])) for i in range(len(sigmas))], name='mse_sigmas_loss')\n",
    "            loss = mse_m_loss + mse_sigmas_loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_loss(model, other_loss_callable, priors=None, n_examples=1, only_kld=False):\n",
    "    def l(y_true, y_pred):\n",
    "        ms = _maybe_get_layer_attr(model, 'kernel_m') \\\n",
    "                + _maybe_get_layer_attr(model, 'bias_m')\n",
    "        sigmas = _maybe_get_layer_attr(model, 'kernel_sigma') \\\n",
    "                + _maybe_get_layer_attr(model, 'bias_sigma')\n",
    "        if priors is None:\n",
    "            ms_prior = [\n",
    "                tf.constant(1, dtype=v.dtype, shape=v.shape)\n",
    "                for v in ms]\n",
    "            sigmas_prior = [\n",
    "                tf.constant(1, dtype=v.dtype, shape=v.shape)\n",
    "                for v in ms]\n",
    "        else:\n",
    "            ms_prior = [\n",
    "                tf.Variable(tf.convert_to_tensor(self.priors[1][i]), trainable=False) \n",
    "                for i, _ in enumerate(ms)]\n",
    "            sigmas_prior = [\n",
    "                tf.Variable(tf.convert_to_tensor(self.priors[1][i]), trainable=False)\n",
    "                for i, _ in enumerate(ms)]\n",
    "            \n",
    "        nll_loss = tf.reduce_mean(other_loss_callable(y_true=y_true, y_pred=y_pred), axis=0)\n",
    "        l2_loss = tf.add_n(\n",
    "            [\n",
    "                tf.reduce_sum((tf.square(ms[i] - ms_prior[i])) / ((tf.square(sigmas_prior[i]) + 1e-8) * 2.0)) \n",
    "                for i, _ in enumerate(ms)], name='l2_loss')\n",
    "        \n",
    "        sigma_squared_loss = tf.add_n([tf.reduce_sum(tf.square(sigmas[i]) / ((tf.square(sigmas_prior[i]) + 1e-8) * 2.0)) for i in range(len(sigmas))], name='sigma_squared_loss')\n",
    "        log_sigma_loss = tf.add_n([tf.reduce_sum(tf.log(v+1e-8)) for v in sigmas],name='log_sigmas_loss')\n",
    "        \n",
    "        if not only_kld:\n",
    "            loss = nll_loss + (l2_loss + sigma_squared_loss - log_sigma_loss) / float(n_examples)\n",
    "        else:\n",
    "            mse_m_loss = tf.add_n([tf.reduce_sum(tf.square(ms[i] - ms_prior[i])) for i in range(len(ms))], name='mse_m_loss')\n",
    "            mse_sigmas_loss = tf.add_n([tf.reduce_sum(tf.square(sigmas[i] - sigmas_prior[i])) for i in range(len(sigmas))], name='mse_sigmas_loss')\n",
    "            loss = mse_m_loss + mse_sigmas_loss\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(tf.keras.optimizers.Adam(1e-04), loss=variational_loss(model, nobrainer.losses.jaccard))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(dataset, steps=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = (outputs > 0.3).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
